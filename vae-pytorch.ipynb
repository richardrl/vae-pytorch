{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import inspect\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.modules.activation as A\n",
    "from tqdm import tqdm\n",
    "%pylab inline\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from livelossplot import PlotLosses\n",
    "import os, errno\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist_data = torchvision.datasets.MNIST('../datasets/mnist', download=True, train=True,\n",
    "                                           transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "#                            transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ]))\n",
    "mnist_loader = torch.utils.data.DataLoader(mnist_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list(mnist_loader)[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module): \n",
    "    def __init__(self, nz=100):\n",
    "        \"\"\"\n",
    "        Encoder(z|X) = Q(z|X) = N(z|mu(X;theta), Sigma(X;theta))\n",
    "        \n",
    "        Input \n",
    "        - X: datapoint\n",
    "        \n",
    "        Output\n",
    "        - mu_z(X): means\n",
    "        - Sigma_z(X): variances\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # Project image \n",
    "            nn.Conv2d(1, 64, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Conv1\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            # Conv2\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.BatchNorm2d(256),\n",
    "        )\n",
    "        self.fc11 = nn.Linear(2304, nz)\n",
    "        self.fc12 = nn.Linear(2304, nz)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.features(X)\n",
    "        X = X.view(X.size(0), 2304)\n",
    "        return self.fc11(X), self.fc12(X)\n",
    "     \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, cz=100):\n",
    "        \"\"\"\n",
    "        P(X|z;theta) = N(X|f(z;theta), sigma^2 * I)\n",
    "        \n",
    "        Input\n",
    "        - z: noise\n",
    "        \n",
    "        Output:\n",
    "        - mu_X(z): mean of gaussian X. Shape: [flatten(X), 1]\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        layer_list = [\n",
    "            nn.ConvTranspose2d(cz, 256, 4, 1, 0, bias=False),\n",
    "            A.ReLU(True),\n",
    "            nn.BatchNorm2d(256),\n",
    "            # Conv1. (?, channels[0], 4, 4) -> (?, channels[1], 8, 8)\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            A.ReLU(True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            # Conv2. (?, channels[1], 8, 8) -> (?, channels[2], 16, 16)\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            A.ReLU(True),\n",
    "            # Conv3. 16x16 -> 28x28\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ConvTranspose2d(64, 1, 4, 2, 3, bias=False),\n",
    "            A.Sigmoid()\n",
    "        ]\n",
    "        \n",
    "        self.model = nn.Sequential(*layer_list)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        return self.model(z.view(z.size(0), z.size(1), 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q = Encoder(nz=100)\n",
    "D = Decoder()\n",
    "\n",
    "def KL_divergence_gaussians(noise_mus, noise_sigmas):\n",
    "    \"\"\"\n",
    "    From Appendix B: https://arxiv.org/pdf/1312.6114.pdf\n",
    "    \"\"\"    \n",
    "#     Q_z_X = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "#     loc, covariance_matrix=None)\n",
    "#     P_z = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "#     torch.zeros(), covariance_matrix=torch.eye())\n",
    "#     Q_loss = torch.distributions.kl.kl_divergence(Q_z_X, P_z)\n",
    "\n",
    "    return .5 * torch.sum(1 + torch.log(noise_sigmas**2) - noise_mus**2 - noise_sigmas**2) \n",
    "\n",
    "def sample_noise(z_mus, z_sigmas):\n",
    "#     import ipdb; ipdb.set_trace()\n",
    "    eps = torch.randn_like(z_mus)*z_sigmas\n",
    "    return eps+z_mus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:00,  2.76it/s]\u001b[A\n",
      "2it [00:00,  3.14it/s]\u001b[A\n",
      "3it [00:00,  3.29it/s]\u001b[A\n",
      "4it [00:01,  3.41it/s]\u001b[A\n",
      "5it [00:01,  3.51it/s]\u001b[A\n",
      "6it [00:01,  3.52it/s]\u001b[A\n",
      "7it [00:02,  3.46it/s]\u001b[A\n",
      "8it [00:02,  3.52it/s]\u001b[A\n",
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/Users/richard/anaconda3/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      "1875it [09:42,  3.22it/s]\n",
      "1875it [09:11,  3.40it/s]\n",
      "210it [01:00,  3.48it/s]"
     ]
    }
   ],
   "source": [
    "fixed_noise = torch.randn([32, 100, 1, 1])\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "opt = optim.Adam(list(Q.parameters()) + list(D.parameters()), lr=0.0002, betas=(.5, .999))\n",
    "for epoch in range(5):\n",
    "    for batch_idx, X in tqdm(enumerate(mnist_loader)):\n",
    "        X = X[0]\n",
    "        X = X.to(device, dtype=torch.float32)\n",
    "        batch_size = X.shape[0]\n",
    "        \n",
    "        z_mus, z_sigmas = Q(X)\n",
    "        \n",
    "        z = sample_noise(z_mus, z_sigmas) # Should be 32x100... \n",
    "        \n",
    "        Q.zero_grad()       \n",
    "        Q_loss = KL_divergence_gaussians(z_mus, z_sigmas)\n",
    "\n",
    "        D.zero_grad()\n",
    "        D_loss = F.binary_cross_entropy(D(z), X)\n",
    "        (Q_loss + D_loss).backward()\n",
    "        \n",
    "        opt.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            import torchvision.utils as vutils\n",
    "            gen_img = D(fixed_noise)\n",
    "            vutils.save_image(gen_img.detach(), F\"fake_samples-{epoch}-{batch_idx}.png\", normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.Tensor([1,2,3]).narrow(0, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = torch.Tensor([[[1,1],[1,1]],[[2,2],[2,2]],[[3,3],[3,3]]])\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# n =m.view(1, torch.prod(torch.tensor(m.size())))\n",
    "n = m.view(m.shape[0], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n.view(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q_z_X = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "    torch.ones(3), covariance_matrix=torch.eye(3))\n",
    "P_z = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "    torch.ones(3), covariance_matrix=torch.eye(3))\n",
    "Q_loss = torch.distributions.kl.kl_divergence(Q_z_X, P_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.Tensor([1,2]) * torch.Tensor([2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
